"""
–ú–∏–Ω–∏-RAG —Å–∏—Å—Ç–µ–º–∞ —Å LangChain
–ü–æ–ª–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞ 15 –º–∏–Ω—É—Ç
"""

from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import hashlib
import json

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.document_loaders import TextLoader, UnstructuredMarkdownLoader
import chromadb
from chromadb.config import Settings

@dataclass
class RAGConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã"""
    chunk_size: int = 1000
    chunk_overlap: int = 200
    embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"
    vector_store_path: str = "./chroma_db"
    collection_name: str = "knowledge_base"
    search_k: int = 3
    # Ollama configuration
    llm_model: str = "mistral"  # 3B-7B GPT-like models: mistral, neural-chat, orca-mini:3b, phi
    ollama_base_url: str = "http://localhost:11434"
    llm_temperature: float = 0.1
    generation_mode: str = "simple"  # "simple" (direct) or "chain" (RetrievalQA)

class MiniRAGSystem:
    """–ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –º–∏–Ω–∏-RAG —Å–∏—Å—Ç–µ–º–∞"""
    
    def __init__(self, config: Optional[RAGConfig] = None):
        self.config = config or RAGConfig()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._init_text_splitter()
        self._init_embeddings()
        self._init_vector_store()
        self._init_llm()
        self._init_prompt()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "documents_processed": 0,
            "chunks_created": 0,
            "queries_processed": 0
        }
    
    def _init_text_splitter(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–ø–ª–∏—Ç—Ç–µ—Ä–∞"""
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""],
            length_function=len,
        )
    
    def _init_embeddings(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.config.embedding_model,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
    
    def _init_vector_store(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        # –°–æ–∑–¥–∞–µ–º –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ Chroma
        self.vector_store = Chroma(
            collection_name=self.config.collection_name,
            embedding_function=self.embeddings,
            persist_directory=self.config.vector_store_path,
            client_settings=Settings(
                chroma_db_impl="duckdb+parquet",
                persist_directory=self.config.vector_store_path,
                anonymized_telemetry=False
            )
        )
    
    def _init_llm(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM —Å Ollama (–ª–æ–∫–∞–ª—å–Ω–∞—è self-hosted –º–æ–¥–µ–ª—å)"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Ollama –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π self-hosted –º–æ–¥–µ–ª–∏
            self.llm = Ollama(
                base_url=self.config.ollama_base_url,
                model=self.config.llm_model,
                temperature=self.config.llm_temperature,
                top_k=40,
                top_p=0.9,
            )
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
            print(f"‚úÖ Ollama –ø–æ–¥–∫–ª—é—á–µ–Ω–∞: {self.config.llm_model} @ {self.config.ollama_base_url}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            print("‚ö†Ô∏è  –ü–æ–∂–∞–ª—É–π—Å—Ç–∞ —É–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω–∞:")
            print("    macOS/Linux: ollama serve")
            print("    Windows: –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ Ollama")
            print("    –°–∫–∞—á–∞—Ç—å: https://ollama.ai")
            print(f"    –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: ollama pull {self.config.llm_model}")
            raise
    
    def _init_prompt(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç-—à–∞–±–ª–æ–Ω–∞"""
        self.prompt_template = PromptTemplate(
            input_variables=["context", "question"],
            template="""
–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.
–ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞.
–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —Å–∫–∞–∂–∏ "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏".

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç (–±—É–¥—å –∫—Ä–∞—Ç–æ–∫ –∏ —Ç–æ—á–µ–Ω):
"""
        )

    def _generate_answer_simple(self, context: str, question: str) -> str:
        """
        –ü—Ä–æ—Å—Ç–æ–π –∏ –ª–µ–≥–∫–∏–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –±–µ–∑ RetrievalQA —Ü–µ–ø–æ—á–∫–∏
        –ü—Ä—è–º–æ–π –≤—ã–∑–æ–≤ LLM —Å –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
        """
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ
        prompt = self.prompt_template.format(context=context, question=question)

        # –í—ã–∑—ã–≤–∞–µ–º LLM –Ω–∞–ø—Ä—è–º—É—é
        try:
            # –î–ª—è ChatOpenAI
            if hasattr(self.llm, 'predict'):
                answer = self.llm.predict(prompt)
            # –î–ª—è HuggingFacePipeline –∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π
            else:
                from langchain.schema import HumanMessage
                messages = [HumanMessage(content=prompt)]
                response = self.llm.generate(messages)
                answer = response.generations[0][0].text
        except Exception as e:
            # –ö–∞–∫ fallback –∏—Å–ø–æ–ª—å–∑—É–µ–º invoke –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            try:
                response = self.llm.invoke(prompt)
                if hasattr(response, 'content'):
                    answer = response.content
                else:
                    answer = str(response)
            except Exception as e2:
                answer = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e2)}"

        return answer.strip() if answer else "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"

    
    def add_document(self, text: str, metadata: Optional[Dict] = None) -> List[str]:
        """
        –î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Å–∏—Å—Ç–µ–º—É
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ ID —á–∞–Ω–∫–æ–≤
        """
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        base_metadata = {
            "source": "manual_input",
            "timestamp": datetime.now().isoformat(),
            "doc_hash": hashlib.md5(text.encode()).hexdigest()[:8]
        }
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        if metadata:
            base_metadata.update(metadata)
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç LangChain
        document = Document(
            page_content=text,
            metadata=base_metadata
        )
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.text_splitter.split_documents([document])
        
        # –û–±–æ–≥–∞—â–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–æ–≤
        chunk_ids = []
        for i, chunk in enumerate(chunks):
            chunk_id = f"{base_metadata['doc_hash']}_chunk_{i}"
            chunk.metadata.update({
                "chunk_id": chunk_id,
                "chunk_index": i,
                "total_chunks": len(chunks),
                "parent_doc_hash": base_metadata['doc_hash']
            })
            chunk_ids.append(chunk_id)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.vector_store.add_documents(chunks)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.stats["documents_processed"] += 1
        self.stats["chunks_created"] += len(chunks)
        
        # –ü–µ—Ä—Å–∏—Å—Ç–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        self.vector_store.persist()
        
        return chunk_ids
    
    def add_document_from_file(self, file_path: str, file_type: str = "auto") -> List[str]:
        """–î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ —Ñ–∞–π–ª–∞"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫ –ø–æ —Ç–∏–ø—É —Ñ–∞–π–ª–∞
        if file_type == "auto":
            if file_path.endswith(".md"):
                file_type = "markdown"
            elif file_path.endswith(".txt"):
                file_type = "text"
        
        if file_type == "markdown":
            loader = UnstructuredMarkdownLoader(file_path)
        else:  # text
            loader = TextLoader(file_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        documents = loader.load()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        chunk_ids = []
        for doc in documents:
            doc.metadata["source"] = file_path
            chunk_ids.extend(self.add_document(doc.page_content, doc.metadata))
        
        return chunk_ids
    
    def query(self, question: str, k: Optional[int] = None) -> Dict[str, Any]:
        """
        –ü–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        """
        search_k = k or self.config.search_k

        # 1. –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        relevant_docs = self.vector_store.similarity_search_with_score(
            question,
            k=search_k
        )

        # 2. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_parts = []
        sources = []

        for i, (doc, score) in enumerate(relevant_docs):
            context_parts.append(
                f"[–î–æ–∫—É–º–µ–Ω—Ç {i+1}, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.3f}]:\n{doc.page_content}"
            )
            sources.append({
                "content": doc.page_content[:200] + "...",
                "metadata": doc.metadata,
                "score": float(score)
            })

        context = "\n\n".join(context_parts)

        # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        if self.config.generation_mode == "simple":
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –∏ –ª–µ–≥–∫–∏–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            answer = self._generate_answer_simple(context, question)
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º RetrievalQA —Ü–µ–ø–æ—á–∫—É (–∏—Å—Ö–æ–¥–Ω—ã–π –º–µ—Ç–æ–¥)
            qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=self.vector_store.as_retriever(
                    search_kwargs={"k": search_k}
                ),
                chain_type_kwargs={"prompt": self.prompt_template},
                return_source_documents=True
            )

            result = qa_chain({"query": question})
            answer = result["result"]

        # 4. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
        response = {
            "question": question,
            "answer": answer,
            "sources": sources,
            "generation_mode": self.config.generation_mode,
            "stats": {
                "docs_retrieved": len(relevant_docs),
                "avg_relevance_score": (
                    sum(score for _, score in relevant_docs) / len(relevant_docs)
                    if relevant_docs else 0
                ),
                "context_length": len(context)
            },
            "raw_context": context if len(context) < 1000 else context[:1000] + "..."
        }

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.stats["queries_processed"] += 1

        return response
    
    def search_similar(self, query: str, k: int = 5) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"""
        results = self.vector_store.similarity_search_with_relevance_scores(query, k=k)
        
        formatted_results = []
        for doc, score in results:
            formatted_results.append({
                "content": doc.page_content[:500],
                "metadata": doc.metadata,
                "relevance_score": float(score),
                "source": doc.metadata.get("source", "unknown")
            })
        
        return formatted_results
    
    def delete_document(self, doc_hash: str) -> bool:
        """–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ —Ö—ç—à—É"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            collection = self.vector_store._collection
            results = collection.get(
                where={"parent_doc_hash": doc_hash},
                include=["metadatas", "documents"]
            )
            
            if results["ids"]:
                # –£–¥–∞–ª—è–µ–º —á–∞–Ω–∫–∏
                collection.delete(ids=results["ids"])
                self.vector_store.persist()
                print(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {len(results['ids'])} —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_hash}")
                return True
            else:
                print(f"‚ö†Ô∏è  –î–æ–∫—É–º–µ–Ω—Ç {doc_hash} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return False
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {e}")
            return False
    
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏—Å—Ç–µ–º—ã"""
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        try:
            collection = self.vector_store._collection
            count = collection.count()
        except:
            count = 0
        
        stats = self.stats.copy()
        stats.update({
            "total_chunks_in_store": count,
            "embedding_model": self.config.embedding_model,
            "chunk_size": self.config.chunk_size,
            "vector_store_path": self.config.vector_store_path
        })
        
        return stats
    
    def export_config(self) -> Dict:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å–∏—Å—Ç–µ–º—ã"""
        return {
            "config": self.config.__dict__,
            "stats": self.get_stats(),
            "components": {
                "embeddings": self.config.embedding_model,
                "vector_store": "ChromaDB",
                "llm": self.config.llm_model,
                "text_splitter": "RecursiveCharacterTextSplitter"
            }
        }

# ==================== –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –†–ê–ë–û–¢–´ ====================

def demo_rag_system():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã RAG —Å–∏—Å—Ç–µ–º—ã"""
    print("üöÄ –ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ Mini-RAG —Å–∏—Å—Ç–µ–º—ã...\n")

    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
    config = RAGConfig(
        chunk_size=800,
        chunk_overlap=150,
        embedding_model="sentence-transformers/all-MiniLM-L6-v2",
        llm_model="mistral",  # –°–∞–º–æ—Ö–æ—Å—Ç–µ–¥ –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ Ollama
        ollama_base_url="http://localhost:11434"
    )

    rag = MiniRAGSystem(config)
    print("‚úÖ RAG —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    print(f"üìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: —á–∞–Ω–∫–∏ {config.chunk_size}/{config.chunk_overlap}")
    print(f"ü§ñ –ú–æ–¥–µ–ª—å: {config.embedding_model} + Ollama ({config.llm_model})\n")
    
    # 2. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    test_docs = [
        """
        # –î–µ–ø–ª–æ–π –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤
        
        –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º Kubernetes –¥–ª—è –¥–µ–ø–ª–æ—è –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤.
        –ö–∞–∂–¥—ã–π —Å–µ—Ä–≤–∏—Å —É–ø–∞–∫–æ–≤–∞–Ω –≤ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä.
        
        –ü—Ä–æ—Ü–µ—Å—Å –¥–µ–ø–ª–æ—è:
        1. –°–±–æ—Ä–∫–∞ –æ–±—Ä–∞–∑–∞ Docker
        2. –ü—É—à –≤ Container Registry
        3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞ Kubernetes
        4. Rolling update —á–µ—Ä–µ–∑ Helm
        
        –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ Prometheus –∏ Grafana.
        –õ–æ–≥–∏ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è –≤ ELK —Å—Ç–µ–∫.
        """,
        
        """
        # Onboarding –Ω–æ–≤—ã—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤
        
        –®–∞–≥–∏ –¥–ª—è –Ω–æ–≤–æ–≥–æ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞:
        1. –ü–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–º —Å–∏—Å—Ç–µ–º–∞–º
        2. –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ (Docker, IDE)
        3. –ò–∑—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –ø–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ
        4. –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–∏—Ç—å—Å—è –∫ –∫–æ–º–∞–Ω–¥–∞–º –≤ Slack
        
        –í–∞–∂–Ω—ã–µ —Å—Å—ã–ª–∫–∏:
        - Wiki: https://wiki.company.com
        - GitLab: https://gitlab.company.com
        - Jenkins: https://jenkins.company.com
        
        –ö–æ–Ω—Ç–∞–∫—Ç—ã:
        - –ú–µ–Ω—Ç–æ—Ä: –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤ (ivan@company.com)
        - HR: hr@company.com
        """,
        
        """
        # –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        
        –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º OAuth 2.0 –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏.
        –í—Å–µ —Å–µ—Ä–≤–∏—Å—ã –¥–æ–ª–∂–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–¥–∏–Ω—ã–π Identity Provider.
        
        –†–æ–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π:
        - ADMIN: –ø–æ–ª–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫–æ –≤—Å–µ–º —Å–∏—Å—Ç–µ–º–∞–º
        - DEVELOPER: –¥–æ—Å—Ç—É–ø –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é
        - VIEWER: —Ç–æ–ª—å–∫–æ —á—Ç–µ–Ω–∏–µ
        
        –¢–æ–∫–µ–Ω—ã JWT –∂–∏–≤—É—Ç 24 —á–∞—Å–∞.
        Refresh tokens –∂–∏–≤—É—Ç 30 –¥–Ω–µ–π.
        
        –í–∞–∂–Ω–æ: –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ —Ö—Ä–∞–Ω–∏—Ç–µ —Å–µ–∫—Ä–µ—Ç—ã –≤ –∫–æ–¥–µ!
        –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ HashiCorp Vault –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–µ–∫—Ä–µ—Ç–∞–º–∏.
        """
    ]
    
    print("üìÑ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    for i, doc_text in enumerate(test_docs):
        chunk_ids = rag.add_document(
            doc_text,
            metadata={
                "title": f"Test Doc {i+1}",
                "category": "technical",
                "author": "system"
            }
        )
        print(f"  –î–æ–∫—É–º–µ–Ω—Ç {i+1}: —Å–æ–∑–¥–∞–Ω–æ {len(chunk_ids)} —á–∞–Ω–∫–æ–≤")
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è:")
    stats = rag.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # 3. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
    print("\nüîç –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...\n")

    test_queries = [
        "–ö–∞–∫ –º—ã –¥–µ–ø–ª–æ–∏–º –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å—ã?",
        "–ß—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –Ω–æ–≤–æ–º—É —Å–æ—Ç—Ä—É–¥–Ω–∏–∫—É?",
        "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤ –Ω–∞—à–µ–π —Å–∏—Å—Ç–µ–º–µ?",
        "–ö–∞–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º?"
    ]

    print("=" * 60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–†–û–°–¢–û–ì–û –ú–ï–¢–û–î–ê –ì–ï–ù–ï–†–ê–¶–ò–ò (small LLM)")
    print("=" * 60)
    for query in test_queries[:2]:  # –ü–µ—Ä–≤—ã–µ –¥–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥
        print(f"\n‚ùì –í–æ–ø—Ä–æ—Å: {query}")
        result = rag.query(query, k=2)

        print(f"üí° –û—Ç–≤–µ—Ç: {result['answer'][:150]}...")
        print(f"üìà –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['stats']['avg_relevance_score']:.3f}")
        print(f"‚öôÔ∏è  –†–µ–∂–∏–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {result['generation_mode']}")

        if result['sources']:
            print(f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏:")
            for i, source in enumerate(result['sources'][:2]):
                print(f"  {i+1}. [{source['metadata'].get('title', 'No title')}] "
                      f"(score: {source['score']:.3f})")

    # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ —Ü–µ–ø–æ—á–∫—É –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–ï–¢–û–î–ê –° –¶–ï–ü–û–ß–ö–û–ô (RetrievalQA)")
    print("=" * 60)
    rag.config.generation_mode = "chain"

    for query in test_queries[2:]:  # –°–ª–µ–¥—É—é—â–∏–µ –¥–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ —Ü–µ–ø–æ—á–∫—É
        print(f"\n‚ùì –í–æ–ø—Ä–æ—Å: {query}")
        result = rag.query(query, k=2)

        print(f"üí° –û—Ç–≤–µ—Ç: {result['answer'][:150]}...")
        print(f"üìà –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['stats']['avg_relevance_score']:.3f}")
        print(f"‚öôÔ∏è  –†–µ–∂–∏–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {result['generation_mode']}")

        if result['sources']:
            print(f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏:")
            for i, source in enumerate(result['sources'][:2]):
                print(f"  {i+1}. [{source['metadata'].get('title', 'No title')}] "
                      f"(score: {source['score']:.3f})")
    
    # 4. –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    print("üîé –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    similar = rag.search_similar("–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏", k=2)
    for i, doc in enumerate(similar):
        print(f"  {i+1}. {doc['content'][:100]}... (score: {doc['relevance_score']:.3f})")
    
    # 5. –≠–∫—Å–ø–æ—Ä—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    print("\nüíæ –≠–∫—Å–ø–æ—Ä—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã...")
    config_export = rag.export_config()
    print(f"  –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {config_export['components']['embeddings']}")
    print(f"  –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {config_export['stats']['total_chunks_in_store']}")
    print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {config_export['stats']['queries_processed']}")
    
    print("\nüéâ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    return rag

# ==================== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ====================

def test_edge_cases(rag: MiniRAGSystem):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ edge cases"""
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ edge cases...")
    
    # 1. –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
    print("1. –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å:")
    result = rag.query("")
    print(f"   –û—Ç–≤–µ—Ç: {result['answer'][:50]}...")
    
    # 2. –ó–∞–ø—Ä–æ—Å –±–µ–∑ –æ—Ç–≤–µ—Ç–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
    print("\n2. –ó–∞–ø—Ä–æ—Å –±–µ–∑ –æ—Ç–≤–µ—Ç–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö:")
    result = rag.query("–ö–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å –ø–∏—Ü—Ü—É?")
    print(f"   –û—Ç–≤–µ—Ç: {result['answer']}")
    
    # 3. –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
    print("\n3. –î–ª–∏–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å:")
    long_query = "–ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å " + " –∏ ".join([f"—Å–∏—Å—Ç–µ–º—É{i}" for i in range(10)])
    result = rag.query(long_query)
    print(f"   –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {result['stats']['docs_retrieved']}")
    
    # 4. –£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    print("\n4. –£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞:")
    # –°–Ω–∞—á–∞–ª–∞ –Ω–∞–π–¥–µ–º —Ö—ç—à –¥–æ–∫—É–º–µ–Ω—Ç–∞
    test_query = "–¥–µ–ø–ª–æ–π –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤"
    similar = rag.search_similar(test_query, k=1)
    if similar:
        doc_hash = similar[0]['metadata'].get('parent_doc_hash')
        if doc_hash:
            success = rag.delete_document(doc_hash)
            print(f"   –£–¥–∞–ª–µ–Ω–∏–µ {'—É—Å–ø–µ—à–Ω–æ' if success else '–Ω–µ—É–¥–∞—á–Ω–æ'}")
    
    print("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ edge cases –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

# ==================== –ó–ê–ü–£–°–ö ====================

if __name__ == "__main__":
    print("=" * 60)
    print("MINI-RAG SYSTEM WITH LANGCHAIN")
    print("–ü–æ–ª–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è 30-–º–∏–Ω—É—Ç–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤—å—é")
    print("=" * 60)
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
    rag_system = demo_rag_system()
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ edge cases (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    # test_edge_cases(rag_system)
    
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–∑–≤–Ω–µ
    print("\nüìã –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ production:")
    print("""
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å self-hosted –º–æ–¥–µ–ª—å—é —á–µ—Ä–µ–∑ Ollama
    from langchain.llms import Ollama

    config = RAGConfig(
        llm_model="mistral",  # –∏–ª–∏ –¥—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å
        ollama_base_url="http://localhost:11434"
    )
    rag = MiniRAGSystem(config)

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    rag.add_document("–í–∞—à —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞", metadata={"source": "api"})

    # –ü–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞
    result = rag.query("–í–∞—à –≤–æ–ø—Ä–æ—Å")
    print(result['answer'])

    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    stats = rag.get_stats()
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['queries_processed']}")

    # –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ Ollama (3B-7B GPT-like):
    # - mistral (7B, –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ –∏ —Ö–æ—Ä–æ—à–æ)
    # - neural-chat (7B)
    # - orca-mini:3b (3B, –∫–æ–º–ø–∞–∫—Ç–Ω–æ)
    # - phi (2.7B, –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ)
    """)